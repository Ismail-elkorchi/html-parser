# ADR-022: Performance stability baseline calibration

## Context
- Release scoring includes a performance component (`weights.performance = 15`).
- Previous `performanceBaseline` values were placeholder constants and not derived from repository benchmark evidence.
- Single-run benchmark evidence is noisy and can produce unstable score movement.

## Decision
- Add `reports/bench-stability.json` generated by a subprocess-isolated multi-run benchmark (`test:bench:stability`).
- In `release` profile:
  - require benchmark stability evidence (`requireBenchStability = true`)
  - require `benchStabilityRuns = 9`
  - score performance from `bench-stability` medians (`source = bench-stability.median`).
- Calibrate `performanceBaseline` from measured `bench-stability` medians using `postGcHeapUsed`.
- Add gate `G-115` to enforce:
  - stability report presence and minimum run count
  - robust spread bounds for throughput and memory
  - median ratio bounds versus calibrated baseline.

## Alternatives considered
- Keep placeholder baseline and single-run bench scoring:
  - rejected because score is not tied to stable, reproducible evidence.
- Remove performance scoring from release:
  - rejected because release policy keeps performance as a non-zero weighted signal.

## Consequences
- Release score reflects calibrated and reproducible benchmark evidence.
- Performance regressions are detected by both score degradation and `G-115` hard gate failures.
- CI behavior is unchanged (`requireBenchStability = false`).

## Validation plan
- Run:
  - `npm run test:bench:stability -- --runs=9`
  - `npm run eval:release`
- Confirm:
  - `reports/bench-stability.json` exists with `runs >= 9`
  - `reports/gates.json` has `G-115` pass
  - `reports/score.json.breakdown.performance.details.source = "bench-stability.median"`.

## Rollback plan
- Revert this ADR's implementation commit.
- Restore prior `evaluation.config.json`, `run-eval`, `score`, and `check-gates` behavior.
